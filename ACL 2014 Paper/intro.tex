%!TEX root = acl2014.tex
\section{introduction}
Distributed word representation embeds a word into a continuous low dimentional vector and can better uncover both the semantic and syntactic information over traditional bag of words representations. It has been successfully applied in many downstream NLP tasks as input features, such as named entity recognition, sentiment analysis, and question answering. Among many embeding methods, CBOW and Skip-Gram model are very popular due to their simplicity and efficiency, making it feasible to learn good embeddings from a large scale training corpus. 

Despite the success and popularity of word embeddings, most of the existing methods treat each word as the mininum unit, which ignores the  morphological inormation of words. The representation of rare words may be poor despite that the training process of CBOW and Skip Gram typically subsample the frequent words. To address this issue, many recent works have investigated how to leverage morphological inormation to learn  better word embeddings. It has been proved efficive to improve word embedding quality, especially for morphologically rich languages.

Chinese is a kind of hieroglyphs and contains rich morphological information. The characters compsing a word can indicate the semantic meaning of the word and words sharing same character components always have similar meanings. Moreover, chinese characters can be broken into fine-grained components, which can be roughly divided into two types: semantic component and phonetic component. The semantic component represents the meaning of a character while the phonetic component represents the sound of a character. 
%For example, 氵is the semantic component of characters 河, 海 , 马  is the phonetic component of 妈 骂.

Leveraging these subword informations such as characters and character components can enrich chinese word embeddings with internal morphological semantics. Some methods have been proposed to incorporate these subword information for chinese word embeddings.  Sun Y et al. 2014 first introduced a radical-enhaced chinese character embedding model based on C\&W model and and  apply  it  on  Chinese  charac-ter similarity judgement and Chinese word segmentation. Yanran Li 2015 et al. developed two component-enhanced Chinese character embedding models and their bi-gram extensions based on CBOW and Skip Gram model.  Chen et al. proposed CWE model to joint learn chinsed word and character embedding and utilize the chinese characters to enrich chinese word embeddings. Jian Xu et al 2016 extends CWE work by exploiting the internal sematic similarity between a word and its characters  to combine word and character embedding  in a cross-lingual fashion. To combine the radical-character, character-word composition information, Rongchao Yin et al. 2016 propose multi-granularity embedding (MGE) model based on CWE model, which repsents the context as the combination of surrounding words, surrounding characters and the radical of target word to predict the target word. 

However, previous works only use character or radical of character to enrich a word embedding and don't make full use of fined grained components of characters. The component
%(部件构造)
 of chinese character is different from radicals, they are sometimes wrongly considered the same. Essentially, radicals are a specific set of characters that are used to index Chinese characters in dictionaries. Although many of them (not all) are also semantic components, each character has only one radical, which can not fully uncover the semantic and structure of a character. Besides, there are about 200 radicals while the number of components is over 10000.  Xinlei Shi et al. 2016 cut a character into fine-grained components according to wubi input method and get a these component embeddings by CBOW model. Then feed these component embeddings into deep neural networks and achives promising results in short-text categorization, chinese word segmentation, web search ranking tasks.

In this work, we present a model to jointly learn chinese word, character, sub character components embeddings. The learned chinese word embeddings can not only leverage the external context concurrence information but also incorporate rich internal subword structure and semantic information. Expreiments on both word similarity and word analogy tasks demonstrate the effectiveness of our model over previous works.